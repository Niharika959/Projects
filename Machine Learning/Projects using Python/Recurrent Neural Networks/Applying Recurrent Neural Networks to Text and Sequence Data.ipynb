{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment 3-Text and Sequence Data**"
      ],
      "metadata": {
        "id": "YA1zL8aB__Bq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem Statement:** Apply Recurrent Neural Networks to Text and Sequence Data. Consider both a embedding layer, and a pretrained word embedding. Which approach \n",
        "did better? Demonstrating how to improve performance of the network, especially when dealing with \n",
        "limited data"
      ],
      "metadata": {
        "id": "k09wyr5rRHLt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model with Embedding Layers**"
      ],
      "metadata": {
        "id": "BeutYV5xAUI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "keras.__version__\n",
        "from keras.layers import Embedding\n",
        "\n",
        "# The Embedding layer takes at least two arguments:\n",
        "# the number of possible tokens, here 1000 (1 + maximum word index),\n",
        "# and the dimensionality of the embeddings, here 64.\n",
        "embedding_layer = Embedding(1000, 64)\n"
      ],
      "metadata": {
        "id": "7p8PLGewnlZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import imdb\n",
        "from keras import preprocessing\n",
        "from keras.utils import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Number of words to consider as features\n",
        "max_features = 10000\n",
        "# Cut texts after this number of words \n",
        "# (among top max_features most common words)\n",
        "maxlen = 200\n",
        "\n",
        "# Load the data as lists of integers.\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "# This turns our lists of integers\n",
        "# into a 2D integer tensor of shape `(samples, maxlen)`\n",
        "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test =  pad_sequences(x_test, maxlen=maxlen)\n",
        "\n",
        "# Combining the Training and Testing data create an entire dataset\n",
        "texts = np.concatenate((x_train, x_test), axis=0)\n",
        "labels = np.concatenate((y_train, y_test), axis=0)\n",
        "\n",
        "# Splitting the data into Training and Validation Samples\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, train_size=20000, test_size=10000, random_state=42, stratify=labels)\n",
        "\n",
        "# Further split the data to get the test size of 5000 samples\n",
        "_, test_texts, _, test_labels = train_test_split(x_test, y_test, test_size=5000, random_state=42, stratify=y_test)"
      ],
      "metadata": {
        "id": "vcKOFhG0npVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten, Dense\n",
        "\n",
        "model = Sequential()\n",
        "# We specify the maximum input length to our Embedding layer\n",
        "# so we can later flatten the embedded inputs\n",
        "model.add(Embedding(10000, 8, input_length=maxlen))\n",
        "# After the Embedding layer, \n",
        "# our activations have shape `(samples, maxlen, 8)`.\n",
        "\n",
        "# We flatten the 3D tensor of embeddings \n",
        "# into a 2D tensor of shape `(samples, maxlen * 8)`\n",
        "model.add(Flatten())\n",
        "\n",
        "# We add the classifier on top\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=5,\n",
        "                    batch_size=32,\n",
        "                    validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuISXTUioMc6",
        "outputId": "4c14bb0d-1f1a-4704-a01d-1d6804f9d906"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_18 (Embedding)    (None, 200, 8)            80000     \n",
            "                                                                 \n",
            " flatten_16 (Flatten)        (None, 1600)              0         \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 1)                 1601      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 81,601\n",
            "Trainable params: 81,601\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "625/625 [==============================] - 34s 53ms/step - loss: 0.6036 - acc: 0.6909 - val_loss: 0.4225 - val_acc: 0.8324\n",
            "Epoch 2/5\n",
            "625/625 [==============================] - 7s 12ms/step - loss: 0.3264 - acc: 0.8704 - val_loss: 0.3106 - val_acc: 0.8742\n",
            "Epoch 3/5\n",
            "625/625 [==============================] - 7s 11ms/step - loss: 0.2467 - acc: 0.9028 - val_loss: 0.2930 - val_acc: 0.8774\n",
            "Epoch 4/5\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2101 - acc: 0.9193 - val_loss: 0.2875 - val_acc: 0.8792\n",
            "Epoch 5/5\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.1832 - acc: 0.9323 - val_loss: 0.2968 - val_acc: 0.8786\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "Vwu4t0xLqK7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using Pre-Trained Word Embeddings**"
      ],
      "metadata": {
        "id": "xvq4mtQ617Ec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz\n",
        "\n",
        "!rm -r aclImdb/train/unsup\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gN2TxIx5wDPQ",
        "outputId": "cbe10aa3-c7c7-4d1a-8735-263316fb9fa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  34.9M      0  0:00:02  0:00:02 --:--:-- 34.9M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Downloading the IMDB data as raw data**"
      ],
      "metadata": {
        "id": "jfOEFDaxBI3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/\n",
        "import os"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_7tT9t52dDJ",
        "outputId": "f0f6502b-e012-4285-f4f0-5ac6d0bce7f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imdb_dir = '/content/drive/MyDrive/aclImdb'\n",
        "train_dir = os.path.join(imdb_dir, 'train')\n",
        "\n",
        "labels = []\n",
        "texts = []\n",
        "\n"
      ],
      "metadata": {
        "id": "IXLm3gaP0mAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for label_type in ['neg', 'pos']:\n",
        "    dir_name = os.path.join(train_dir, label_type)\n",
        "    for fname in os.listdir(dir_name):\n",
        "        if fname[-4:] == '.txt':\n",
        "            f = open(os.path.join(dir_name, fname))\n",
        "            texts.append(f.read())\n",
        "            f.close()\n",
        "            if label_type == 'neg':\n",
        "                labels.append(0)\n",
        "            else:\n",
        "                labels.append(1)\n",
        "\n",
        "maxlen = 150  # cutting off reviews after 150 words\n",
        "training_samples = 1000  # training on 1000 samples\n",
        "validation_samples = 10000  # validating on 10000 samples\n",
        "max_words = 10000  # considering the top 10,000 words in the dataset\n"
      ],
      "metadata": {
        "id": "IDYxgows7Yw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenizing the Data**"
      ],
      "metadata": {
        "id": "1H2ASTk7BWkE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "labels = np.asarray(labels)\n",
        "print('Shape of data tensor:', data.shape)\n",
        "print('Shape of label tensor:', labels.shape)\n",
        "# Splitting the data into a training set and a validation set\n",
        "indices = np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "\n",
        "x_train = data[:training_samples]\n",
        "y_train = labels[:training_samples]\n",
        "x_val = data[training_samples: training_samples + validation_samples]\n",
        "y_val = labels[training_samples: training_samples + validation_samples]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjrzdfv9_LyD",
        "outputId": "501f27b9-915d-4932-f9c4-fd14679e8f0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 59382 unique tokens.\n",
            "Shape of data tensor: (11572, 150)\n",
            "Shape of label tensor: (11572,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Downloading the GloVe Embeddings**\n",
        "\n",
        "Pre-Processing the embeddings\n"
      ],
      "metadata": {
        "id": "KStb0Vr9Bf5P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "glove_dir = '/content/drive/MyDrive/glove.6B'\n",
        "\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1K6396RbojrL",
        "outputId": "7df55e93-e8a4-49a3-b6f3-adb90d21dd89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 100\n",
        "\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if i < max_words:\n",
        "        if embedding_vector is not None:\n",
        "            # Words not found in embedding index will be all-zeros.\n",
        "            embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "yh_MIjaZB_MC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Defining a Model**"
      ],
      "metadata": {
        "id": "-ymOjynuBt1M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Flatten, Dense,Dropout,LSTM\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
        "\n",
        "model.add(LSTM(128))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = False\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZaTbG42CCB_x",
        "outputId": "ee573ab2-3ec6-4225-fa9f-9016e1a9af2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_8 (Embedding)     (None, 150, 100)          1000000   \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 128)               117248    \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 256)               33024     \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 1)                 257       \n",
            "                                                                 \n",
            " flatten_6 (Flatten)         (None, 1)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,150,529\n",
            "Trainable params: 150,529\n",
            "Non-trainable params: 1,000,000\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading GloVe embeddings in the model**"
      ],
      "metadata": {
        "id": "E-DwOd33CCeG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = False"
      ],
      "metadata": {
        "id": "4PQyTST9CG8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training and Evaluating the Model**"
      ],
      "metadata": {
        "id": "6GN27Y16CLP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=5,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(x_val, y_val))\n",
        "model.save_weights('pre_trained_glove_model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfFtN3CcCJxD",
        "outputId": "06d1e310-d2a8-41b9-b63c-cadb8b8e3fea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "32/32 [==============================] - 6s 65ms/step - loss: 0.0381 - acc: 0.9880 - val_loss: 3.3340e-04 - val_acc: 1.0000\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 2s 50ms/step - loss: 2.6949e-04 - acc: 1.0000 - val_loss: 7.6711e-05 - val_acc: 1.0000\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 3s 94ms/step - loss: 9.1294e-05 - acc: 1.0000 - val_loss: 2.9142e-05 - val_acc: 1.0000\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 1s 47ms/step - loss: 4.0504e-05 - acc: 1.0000 - val_loss: 1.5665e-05 - val_acc: 1.0000\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 2s 49ms/step - loss: 2.6709e-05 - acc: 1.0000 - val_loss: 1.0021e-05 - val_acc: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kfcn_G0cCk0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing the Model**"
      ],
      "metadata": {
        "id": "rNe7R1u0CWCa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_dir = os.path.join(imdb_dir, 'test')\n",
        "\n",
        "labels = []\n",
        "texts = []\n",
        "\n",
        "for label_type in ['neg', 'pos']:\n",
        "    dir_name = os.path.join(test_dir, label_type)\n",
        "    for fname in sorted(os.listdir(dir_name)):\n",
        "        if fname[-4:] == '.txt':\n",
        "            f = open(os.path.join(dir_name, fname))\n",
        "            texts.append(f.read())\n",
        "            f.close()\n",
        "            if label_type == 'neg':\n",
        "                labels.append(0)\n",
        "            else:\n",
        "                labels.append(1)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "x_test = pad_sequences(sequences, maxlen=maxlen)\n",
        "y_test = np.asarray(labels)"
      ],
      "metadata": {
        "id": "QWV9CVU6Cz1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights('pre_trained_glove_model.h5')\n",
        "model.evaluate(x_test, y_test)\n"
      ],
      "metadata": {
        "id": "a1r0LXP5Dbpi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}