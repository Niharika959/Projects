---
title: "Using Dbscan clustering to analyze U.S Government Power Supply"
output:
  html_document:
    df_print: paged
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
```
***Problem Statement:***
*Use Machine Learning Algorithm to analyze power supply of U.S Government.*


***Loading Required Libraries:***
```{r}
library(dplyr)
library(caret)
library(factoextra)
library(leaps)
library(dbscan)
library(esquisse)
```

**Data Cleansing:**
```{r}
Data<-read.csv("./fuel_receipts_costs_eia923.csv")

#Replacing missing values with NA

Na<- Data %>% replace(.=="",NA)

#Getting the percentages of the null values in each column:
missing_values<- (colMeans(is.na(Na))*100)

#Removing variables with null values having percentage more than 50 percent and few other variables which doesn't add much contribution to the analysis:

Data_1<- subset(Data,select=-c(1:5,7:8,12:14,22:25,26:30))
```


```{r}
#Random sampling of 2% data:
set.seed(2467)
data_2<-sample_n(Data_1,12000)
```


**Data Exploration:**
```{r}
#Converting fuel_type_code_pudl into numerical data by creating dummy variables:

fuel_type_coal <- ifelse(data_2$fuel_type_code_pudl=="coal" ,1,0)
fuel_type_gas <- ifelse(data_2$fuel_type_code_pudl=="gas" ,1,0)
fuel_type_oil <- ifelse(data_2$fuel_type_code_pudl=="oil" ,1,0)

#Appending these new columns with the existing dataframe:

New_data<- cbind(data_2[,-3],fuel_type_coal,fuel_type_gas,fuel_type_oil)

```

**Data Preparation:**
```{r}
#Splitting data into training and test:

Split_data<-createDataPartition(New_data$fuel_received_units,p=.75,list=FALSE)
Training<-New_data[Split_data,]
Test<-New_data[-Split_data,]

Training[is.na(Training)] <- 0
Test[is.na(Test)] <- 0


```

**Modeling Strategy:**

*Initial approach was to choose k-means algorithm. However, after noticing that the clusters formed in k-means are overlapping which meant that the data has border points and outliers. Since the variation between the clusters was too small, I did not prefer to continue my analysis with those clusters.*

*The next immediate idea was to perform DBSCAN algorithm as it handles border points and outliers.*
```{r}

#Selecting numerical data to form clusters:

Training_numerical<-Training[,c(4:9,11:13)]

#Normalizing the data:
Training_norm<-scale(Training_numerical)
```


```{r}

dbscan::kNNdistplot(Training_norm, k =  2)
abline(h = 0.5,col="red")

```


*Choosing epsilon value to be 0.5 based on the above plot. The choice of minPts has been made after few experiments being done with other values. When minPts is assigned to be 100, I was getting perfect 3 clusters with the variation between clusters being more.*
```{r}
db <- dbscan::dbscan(Training_norm, eps = 0.5, minPts = 100)
db
```
*There are 914 border points in the data and 3 clusters have been formed with 4732,740 and 2614 data points in each cluster respectively.*

```{r}

#Plotting the clusters for better data visualization:
fviz_cluster(db,Training_numerical,main="3 clusters")

#Assigning clusters to the original data:
assigned_data<-cbind(Training_numerical,db$cluster)


#Finding mean within each cluster to interpret the clusters:
mean_k3 <- Training_numerical %>% mutate(Cluster = db$cluster) %>% group_by(Cluster) %>% summarise_all("mean")
head(mean_k3)
```
**Interpretation of each cluster:**

*It can be observed that each of the fuel type falls under each cluster. Therefore, my analysis of each cluster is based on fuel types.*

**Names of each cluster:**

*Cluster 1: Gas*

*Cluster 2: Oil*

*Cluster 3: Coal*


**Interpreting the pattern in the clusters with respect to the Categorical variables:**
```{r}

plots <- Training[,c(1:3,10)] %>% mutate(Clusters=db$cluster)

ggplot(plots, mapping = aes(factor(Clusters), fill =contract_type_code_label))+geom_bar(position='dodge')+labs(x ='Clusters')

ggplot(plots, mapping = aes(factor(Clusters), fill =energy_source_code_label))+geom_bar(position='dodge')+labs(x ='Clusters')

ggplot(plots, mapping = aes(factor(Clusters), fill =fuel_group_code))+geom_bar(position='dodge')+labs(x ='Clusters')

ggplot(plots, mapping = aes(factor(Clusters), fill =primary_transportation_mode_code))+geom_bar(position='dodge')+labs(x ='Clusters')
```



**Analysis of Cluster 1: Gas**

*Gas has the average lowest fuel cost per mmbtu. That also explains why is it supplied the most number of avergae units of fuel.*
*Gas does not contain any ash,sulfur and mercury content which makes it a good type of fuel that can be used.*
*It also contains the lowest average of fuel mmbtu per unit. which means that the heat content generated by fuel is less*
*Based on the graph, most gas type fuel is purchased on spot and a relatively lesser amount is purchased on contract.*
*Energy source code is Natural gas and the most commonly used transportation type to supply this type of fuel is through pipelines(PL).*

**Analysis of Cluster 2: Oil**

*The average cost of oil per mmbtu is 10.49, making it the most expensive type of fuel in the USA.*
*The average units of oil received in comparision to gas and coal is very less, probably because it is the most expensive fuel type.*
*Even oil doesnot contain ash and mercury percentage but do has a little percent of sulfur content.*
*From the graphs, oil is only purchased on spot. No contract based purchases have been recorded.*
*The energy source code for this type of fuel is DFO which means Distillate Fuel Oil which also includes*


**Analysis of Cluster 3: Coal**
*Coal is the least expensive type of fuel and is also widely supplied in the USA.*
*Unlike other two fuels, it contains ash,sulfur and mercury content.*
*The average heat energy received from coal is 21.5612.*
*From the graphs of categorical variables, coal is purchased mostly on spot.*
*The energy source code for this type of fuel is BIT and SUB, which indicates that conventional Steam Coal is supplied the most in the U.S.A*



\newpage

```{r}

#Running multiple linear regression model to determine the best set of variables to predict fuel_cost_per_mmbtu by considering variables which were used to form clusters:

Model<- lm(Training_numerical$fuel_cost_per_mmbtu~.,data=Training_numerical)
summary(Model)

#Fuel received units,fuel_type_coal and fuel_type_oil best determine the fuel_cost_per_mmbtu variable.


#Checking the prediction of the above model on Test data
Test_data<- Test[,c(4:9,11:13)]
Test_Model<-predict(Model, data = Test_data)

```


```{r}
#Predicting clusters for Test data:
Test_norm<-scale(Test_data)

Testing_clusters<- predict(db,newdata = Test_norm,data=Training_norm)


#Appending cluster information and above predicted fuel cost per unit values to the test data:
Test_predicted_data<- cbind(Test_data,Test_Model,Testing_clusters)
head(Test_predicted_data)

#Finding out the averages to see how close the predicted values are to the actual fuel_cost values:
mean_Predicted_Test <- Test_predicted_data %>% mutate(Cluster = Testing_clusters) %>% group_by(Cluster) %>% summarise_all("mean")
head(mean_Predicted_Test)

```

*We could see that the difference between predicted values and actual values is too high.*
```{r}

#Re-running the cluster information with choosen variables:

Model_new<- lm(Test_predicted_data$fuel_cost_per_mmbtu~Test_predicted_data$fuel_received_units+Test_predicted_data$fuel_type_coal+Test_predicted_data$fuel_type_gas,data=Test_predicted_data)

summary(Model_new)

#Predicting the new model on Test_data again to verify if there is any difference in prediction by choosing the variables with significant importance:

Prediction_on_test<- predict(Model_new,data=Test_data)

#Appending the new values with Test data and cluster information:
Test_predicted_data_2<- cbind(Test_data,Prediction_on_test,Testing_clusters)

#Finding out the averages to compare the actual values and predicted values:
mean_Predicted_Test_2 <- Test_predicted_data_2 %>% mutate(Cluster = Testing_clusters) %>% group_by(Cluster) %>% summarise_all("mean")

head(mean_Predicted_Test_2)

```

*Observations: We could see that the averages of predicted values in each cluster is comparitively closer to the averages of actual fuel cost values. This shows that by choosing variables with significant relationship and cluster information leads to better prediction.*
